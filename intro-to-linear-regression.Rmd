---
title: "introduction-to-linear-regression"
author: "Scott Heng"
date: "8/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Linear Regression

## Theory
Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. The relationships are modeled using linear predictor functions (a set of coefficients and explanatory variables) whose unknown model parameters are estimated from the data, which can consequently be used to predict the outcome of response variable.

**Simple Linear Regression** - when there is one explanatory/predictor variable
**Multiple Linear Regression** - when there is more than one explanatory/predictor variable

Given a data set $\{y_i,x_{i1},x_{i2}, ... ,x_{ip}\}^n_{i=1}$ where there are n data points and p explanatory variables, the relationship can be modeled as 
$$
y_{i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} + \epsilon_{i} = \mathbf{x}^{\mathbf{T}}_i \mathbf{\beta} \; for \ i=1...n
$$

which then n stacked equations together are written in matrix notation as:
$$
\mathbf{y} = \mathbf{X \beta} + \epsilon
$$

### Assumptions

#### Linearity
The relationship between X and the mean of Y is linear.

#### Constant Variance (homoscedasticity)
The variance of the residual is the same for any value of X.

#### Independence
Observations are independent of each other, with little to no evidence of multicollinearity. Multicollinearity is the phenomenon when one explanatory variable can be linearly predicted from the others with a substantial degree. This will only exists in multiple linear regression.

#### Normality
For any fixed value of X, Y is normally distributed





```{r packages include=FALSE}
# install package function that installs the package 
# package still requires loading as library
loadPackage <- function(package) {
  if (!package %in% installed.packages())
    install.packages(package)
}
```

## Interpretations

A fitted linear regression model can be used to identify the relationship between a single predictor variable $x_j$ and the response variable $y$ when all the other predictor variables in the model are "held fixed". Specifically, the interpretation of $\beta_j$ is the expected change in $y$ for a one-unit change in $x_j$ when the other covariates are held fixed.

## Practice Example

### Loading data and packages
```{r load data and packages echo=FALSE}
loadPackage("dplyr")
library(dplyr)
library(datasets)
```
