---
title: "Introduction to Linear Regression"
author: "Scott Heng"
date: "8/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Linear Regression

## Theory
Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables. The relationships are modeled using linear predictor functions (a set of coefficients and explanatory variables) whose unknown model parameters are estimated from the data, which can consequently be used to predict the outcome of response variable.

**Simple Linear Regression** - when there is one explanatory/predictor variable

**Multiple Linear Regression** - when there is more than one explanatory/predictor variable

Given a data set $\{y_i,x_{i1},x_{i2}, ... ,x_{ip}\}^n_{i=1}$ where there are n data points and p explanatory variables, the relationship can be modeled as 
$$
y_{i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} + \epsilon_{i} = \mathbf{x}^{\mathbf{T}}_i \mathbf{\beta} \; for \ i=1...n
$$

which then n stacked equations together are written in matrix notation as:
$$
\mathbf{y} = \mathbf{X \beta} + \epsilon
$$

### Assumptions

#### Linearity
The relationship between X and the mean of Y is linear.

Linearity can be checked by:

1. Residuals vs. predicted plot
- A *Residual* is the vertical distance between a data point and the regression line. Each data point has *one* residual.
- Curved or non-horizontally spread cloud on such a plot is diagnostic for non-linearity. To uncover more hidden (partial) non-linearity plot the residuals against each of the predictors.

#### Constant Variance (homoscedasticity)
The variance of the residual is the same for any value of X. A plot of standardized **residuals versus predicted** values can show whether points are equally distributed across all values of the independent variables.

Homoscedasticity can be checked by:

1. Residuals vs. predicted plot
- There should be no clear pattern in the distribution; if there is a cone-shaped pattern (as shown below), the data is heteroscedastic.


![](resources/residuals_unfixed.png)

#### Independence
Observations are independent of each other, with little to no evidence of multicollinearity. 

**Multicollinearity** is the phenomenon when one explanatory variable can be linearly predicted from the others with a substantial degree. This will only exists in multiple linear regression.

Independence can be checked by:

1. Correlation Matrix
- When computing a matrix of Pearson's bivariate correlations among independent variables, the magnitude of the coefficients should be less than .80

2. Variance Inflation Factor (VIF)
- **VIF** indicates the degree that the variances in the regression estimates are increased due to multicollinearity. VIF values higher than 10 indicate that multicollinearity is a problem

If multicollinearity is found in the data, one possible solution is to center the data. To center the data, subtract the mean score from each observation for each independent variable. The simplest solution, however, is to identify the variables causing multicollinearity and removing those variables from the regression

#### Normality
For any fixed value of X, Y is normally distributed.

Normality can be checked by:

1. Histogram of predicted values
- Histogram should roughly reflect a normal distribution

2. Q-Q Plot
- is a quantile-quantile probability plot; a graphical method for comparing two probability distributions by plotting their quantiles against each other.

- **How it works** The set of intervals for the quantiles is chosen. A point (x,y) on the plot corresponds to one of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.   

*better understand Q-Q plot | Weibull distribution*

3. Goodness of fit test (e.g. the Kolmogorov-Smirnov test)
- This test must be conducted on the residuals themselves

*the Kolmogorov-Smirnov test*


```{r packages, include=FALSE}
# install package function that installs the package 
# package still requires loading as library
validatePackage <- function(package) {
  if (!package %in% installed.packages())
    install.packages(package)
}
```

## Interpretations

A fitted linear regression model can be used to identify the relationship between a single predictor variable $x_j$ and the response variable $y$ when all the other predictor variables in the model are "held fixed". Specifically, the interpretation of $\beta_j$ is the expected change in $y$ for a one-unit change in $x_j$ when the other covariates are held fixed.

## Practice Example

### Loading data and packages
```{r load data and packages, message=FALSE}
library(dplyr)
library(tidyverse)
library(datasets)
library(caret)
library(car)
```

### Data Preparation
```{r}
data(iris)
head(iris)
summary(iris)
```

### EDA
```{r}
pairs(iris[1:4], main = "Iris Data", pch = 21, bg = c("red", "green3", "blue"))
```
*ggplot histogram, box plot, etc*

```{r}
simple_linear <- lm(Petal.Width ~ Petal.Length, data=iris)
summary(simple_linear)
```

### Building the Model

```{r}
multi_linear <- lm(Petal.Width ~ Petal.Length + Sepal.Length + Sepal.Width + Species, data=iris)
summary(multi_linear)
```

```{r}
# model 2.0

multi_linear2 <- lm(Petal.Width ~Sepal.Length + Sepal.Width + Species, data=iris)
summary(multi_linear2)
```

### Interpretations

**Petal.Width vs Sepal.Width**

Holding all else constant, 1 unit change in Sepal.Width will cause a 0.23237 increase in Petal.Width.

**Petal.Width vs. Virginica**

Holding all else constant, having the flower to be of a virginica species will have a 1.7848 increase in Petal.Width as compared to a flower of a setosa species.

#### Just to Note

**P-Values** 

- the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
- A small p-value (desired) means that such an extreme observed outcome would be very unlikely under the null hypothesis.


**R-squared**

- also known as the coefficient of determination
- values range from 0 to 1
- is the proportion of the variation of the dependent variable that is predictable from the independent variable
- An R-squared of 1 (100%) means that all movements of a security (or another dependent variable(s)) are completely explained by movements in the index (or the independent variable).
- Multiple r-squared vs Adjusted r-squared
  - The fundamental point is that when you add predictors to your model, the multiple r-squared will always increase, as a predictor will always explain some portion of the variance. Adjusted r-squared controls against this increase, and adds penalties for the number of predictors in the model.
  

**F-statistic**

- see ANOVA

### Evaluating The Model

```{r}
# Testing the model

# Splitting the data set
dt = sort(sample(nrow(iris),nrow(iris)*0.7))
train <- iris[dt,]
test <- iris[-dt,]

# Training the model[
final_model <- lm(Petal.Width ~ Sepal.Length + Sepal.Width + Species, data=train)

#predicting values
predicted_vals <- predict(final_model, newdata =test)
```

*how to evaluate predicted vs actual values*

### Validating Assumptions
There are 4 assumptions to be validated:

1. Linearity
2. Homoscedasticity
3. Independence
4. Normality

```{r}
# plotting the residuals vs fitted plot and other validation plots
par(mfrow=c(2,2))
plot(multi_linear)
```

From the Residuals vs Fitted plot, we can see that categorized by Species, the residuals are randomly distributed, satistfying the assumptions of **linearity** and **homodascedasticity**.

```{r}
# correlation matrix
pairs(iris)

# VIF
vif(multi_linear)
```
We plot a correlation matrix between the predictor variables to test for **independence**. We see that there is some collinearity between Petal.Length and Petal Width, but considering Petal. This is further reinforced with high VIF values. Species also has a high VIF value considering it to be a categorical variable. Thus we remove Petal.Length and re-run the model.

```{r}
hist(predicted_vals)
```
